#!/usr/bin/env python3
"""
Quality gate for success stories cases.
Cleans text, generates proper titles/summaries, filters garbage.
"""

import json
import re
from pathlib import Path
from typing import Optional

# Stop phrases for titles
TITLE_STOP_PHRASES = [
    'eb-1a ÐºÐµÐ¹Ñ', 'eb-2 niw ÐºÐµÐ¹Ñ', 'niw ÐºÐµÐ¹Ñ', 'o-1 ÐºÐµÐ¹Ñ', 'o-1a ÐºÐµÐ¹Ñ', 'o-1b ÐºÐµÐ¹Ñ',
    'Ð¿Ñ€Ð¸Ð²ÐµÑ‚', 'Ð²ÑÐµÐ¼ Ð¿Ñ€Ð¸Ð²ÐµÑ‚', 'ÑƒÑ€Ð°', 'Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ', 'Ñ€ÐµÐ±ÑÑ‚Ð°', 'Ñ…Ð¾Ñ‡Ñƒ Ð¿Ð¾Ð´ÐµÐ»Ð¸Ñ‚ÑŒÑÑ',
    'Ð´Ð¾Ð±Ñ€Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ', 'Ð´Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾', 'Ð¼Ð¾Ñ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ', 'Ð´ÐµÐ»ÑŽÑÑŒ', 'approval', 'approved',
    'Ð°Ð¿Ñ€ÑƒÐ²', 'Ð¾Ð´Ð¾Ð±Ñ€Ð¸Ð»Ð¸', 'Ð°Ð¿Ð¿Ñ€ÑƒÐ²'
]

# Greeting patterns to remove from start
GREETING_PATTERNS = [
    r'^(Ð²ÑÐµÐ¼\s+)?Ð¿Ñ€Ð¸Ð²ÐµÑ‚[!.\s]*',
    r'^ÑƒÑ€Ð°[!,.\s]+',
    r'^Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ[!.\s]*',
    r'^Ñ€ÐµÐ±ÑÑ‚Ð°[!,.\s]*',
    r'^(Ñ…Ð¾Ñ‡Ñƒ\s+)?Ð¿Ð¾Ð´ÐµÐ»Ð¸Ñ‚ÑŒÑÑ[!.\s]*',
    r'^Ð´Ð¾Ð±Ñ€(Ñ‹Ð¹|Ð¾Ðµ)\s+(Ð´ÐµÐ½ÑŒ|ÑƒÑ‚Ñ€Ð¾)[!.\s]*',
    r'^Ð¼Ð¾Ñ\s+Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ[!.\s]*',
    r'^Ð´ÐµÐ»ÑŽÑÑŒ[!.\s]*',
    r'^\*+\s*',
]

# Brand names to replace with [ÑÐµÑ€Ð²Ð¸Ñ]
BRAND_NAMES = [
    'passright', 'wegreened', 'idreem', 'prideimmigration', 'pride immigration',
    'immigrationhelp', 'usvisahelp', 'talentpetition', 'ÑˆÐ°Ð¼Ð°ÐµÐ²', 'greencard.pro',
    'lawfirm', 'visalaw'
]


def clean_text(text: str) -> str:
    """Clean text from markdown artifacts, hashtags, placeholders, brands."""
    if not text:
        return ""

    # Remove markdown artifacts
    text = re.sub(r'\*+', '', text)
    text = re.sub(r'__+', '', text)

    # Remove hashtags at start or standalone
    text = re.sub(r'#\w+\s*', '', text)

    # Remove/replace placeholders
    text = re.sub(r'\[name\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\[Ð¸Ð¼Ñ\]', '', text)
    text = re.sub(r'\[ÑÑÑ‹Ð»ÐºÐ°\]', '', text)
    text = re.sub(r'\[Ð°ÐºÐºÐ°ÑƒÐ½Ñ‚\]', '', text)

    # Replace brand names with [ÑÐµÑ€Ð²Ð¸Ñ]
    for brand in BRAND_NAMES:
        text = re.sub(rf'\b{brand}\b', '[ÑÐµÑ€Ð²Ð¸Ñ]', text, flags=re.IGNORECASE)

    # Remove emoji spam
    text = re.sub(r'[â˜ºï¸ðŸ’«ðŸ“–ðŸƒðŸ“ðŸ˜ðŸ™‚ðŸ¥³ðŸ™ðŸŽ‰âœ¨ðŸ’ªðŸ”¥â¤ï¸]+', '', text)

    # Remove greetings from start
    for pattern in GREETING_PATTERNS:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)

    # Clean whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def is_title_garbage(title: str) -> bool:
    """Check if title is too generic or garbage."""
    title_lower = title.lower().strip()

    # Too short
    if len(title_lower) < 5:
        return True

    # Matches stop phrases
    for phrase in TITLE_STOP_PHRASES:
        if title_lower.startswith(phrase) or title_lower == phrase:
            return True

    # Just visa type
    if title_lower in ['eb-1a', 'eb-2 niw', 'eb-2', 'o-1', 'o-1a', 'o-1b', 'niw']:
        return True

    return False


def generate_title(case: dict) -> str:
    """Generate a proper title from case data."""
    visa = case.get('visa', 'EB-1A')
    field = case.get('field', '')
    rfe = case.get('rfe', False)
    premium = case.get('premium', False)
    service_center = case.get('service_center', '')

    parts = [visa]

    if field:
        parts.append(field)

    modifiers = []
    if premium:
        modifiers.append('premium')
    if rfe:
        modifiers.append('RFE')
    if service_center:
        modifiers.append(service_center)

    if modifiers:
        parts.append(', '.join(modifiers))

    title = ': '.join(parts[:2])
    if len(parts) > 2:
        title += f" ({parts[2]})"

    return title[:55]


def extract_summary(context: str, case: dict) -> str:
    """Extract a meaningful summary from context."""
    if not context:
        parts = []
        if case.get('premium'):
            parts.append('Premium processing')
        if case.get('rfe'):
            parts.append('RFE')
        if parts:
            return f"ÐžÐ´Ð¾Ð±Ñ€ÐµÐ½Ð¸Ðµ. {', '.join(parts)}."
        return "ÐžÐ´Ð¾Ð±Ñ€ÐµÐ½Ð¸Ðµ (Ð¿Ð¾ ÑÐ»Ð¾Ð²Ð°Ð¼ Ð°Ð²Ñ‚Ð¾Ñ€Ð°)."

    # Look for sentences with key info
    sentences = re.split(r'[.!?]+', context)

    keywords = ['Ð¾Ð´Ð¾Ð±Ñ€', 'Ð°Ð¿Ñ€ÑƒÐ²', 'Ð°Ð¿Ð¿Ñ€ÑƒÐ²', 'approved', 'rfe', 'noid',
                'premium', 'Ð´Ð½ÐµÐ¹', 'Ð¼ÐµÑÑÑ†', 'nebraska', 'texas', 'Ð½ÐµÐ±Ñ€Ð°ÑÐºÐ°', 'Ñ‚ÐµÑ…Ð°Ñ']

    for sent in sentences:
        sent_lower = sent.lower()
        if any(kw in sent_lower for kw in keywords):
            summary = sent.strip()
            if len(summary) > 20:
                return summary[:100] + ('...' if len(summary) > 100 else '')

    # Fallback: first sentence if long enough
    if sentences and len(sentences[0].strip()) > 20:
        return sentences[0].strip()[:100]

    # Ultimate fallback
    parts = []
    if case.get('premium'):
        parts.append('Premium')
    if case.get('rfe'):
        parts.append('Ð¿Ð¾ÑÐ»Ðµ RFE')
    if case.get('service_center'):
        parts.append(case['service_center'])

    if parts:
        return f"ÐžÐ´Ð¾Ð±Ñ€ÐµÐ½Ð¸Ðµ. {', '.join(parts)}."
    return "ÐžÐ´Ð¾Ð±Ñ€ÐµÐ½Ð¸Ðµ (Ð¿Ð¾ ÑÐ»Ð¾Ð²Ð°Ð¼ Ð°Ð²Ñ‚Ð¾Ñ€Ð°)."


def is_context_duplicate(summary: str, context: str) -> bool:
    """Check if context is essentially the same as summary."""
    if not context or not summary:
        return True

    # Normalize for comparison
    s1 = re.sub(r'\W+', '', summary.lower())
    s2 = re.sub(r'\W+', '', context.lower())

    # If one contains the other or they're very similar
    if s1 in s2 or s2 in s1:
        return True

    # Check similarity ratio
    if len(s1) < 30 and len(s2) < 30:
        return True

    return False


def is_card_garbage(case: dict) -> bool:
    """Check if the entire card has no useful content."""
    title = clean_text(case.get('title', ''))
    context = clean_text(case.get('context', ''))

    # Title is garbage
    title_bad = is_title_garbage(title)

    # Context too short
    context_bad = len(context) < 50

    # No useful metadata
    has_field = bool(case.get('field'))
    has_center = bool(case.get('service_center'))
    has_flags = case.get('rfe') or case.get('premium')

    # If title is bad AND context is bad AND no useful metadata
    if title_bad and context_bad and not has_field and not has_flags:
        return True

    return False


def process_case(case: dict) -> Optional[dict]:
    """Process a single case, returning cleaned version or None if garbage."""
    # Check if entire card is garbage
    if is_card_garbage(case):
        return None

    # Clean texts
    cleaned_title = clean_text(case.get('title', ''))
    cleaned_context = clean_text(case.get('context', ''))

    # Generate title if needed
    if is_title_garbage(cleaned_title):
        cleaned_title = generate_title(case)

    # Generate summary
    summary = extract_summary(cleaned_context, case)

    # Build cleaned case
    cleaned = case.copy()
    cleaned['title'] = cleaned_title
    cleaned['context'] = cleaned_context
    cleaned['summary'] = summary

    # Mark if context should be hidden
    cleaned['hide_context'] = (
        is_context_duplicate(summary, cleaned_context) or
        len(cleaned_context) < 30
    )

    return cleaned


def process_all_cases(input_path: str, output_path: str = None):
    """Process all cases and save cleaned version."""
    with open(input_path) as f:
        data = json.load(f)

    original_count = len(data['cases'])
    cleaned_cases = []
    removed_ids = []

    for case in data['cases']:
        cleaned = process_case(case)
        if cleaned:
            cleaned_cases.append(cleaned)
        else:
            removed_ids.append(case.get('id', 'unknown'))

    data['cases'] = cleaned_cases

    output = output_path or input_path
    with open(output, 'w') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"Processed {original_count} cases")
    print(f"Kept: {len(cleaned_cases)}")
    print(f"Removed: {len(removed_ids)}")
    if removed_ids:
        print(f"Removed IDs: {removed_ids}")

    return data


if __name__ == '__main__':
    import sys
    input_path = sys.argv[1] if len(sys.argv) > 1 else '/Users/aeb/mintlify-docs/data/cases.json'
    process_all_cases(input_path)
